{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import tensorflow as tf  # 2.0\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv3D, MaxPooling3D, Dense, Dropout, Activation, Flatten \n",
    "from keras.layers import Input, concatenate\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from augmentedvolumetricimagegenerator.generator import customImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# Administrative items\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Where the file is located\n",
    "path = r'C:\\Users\\jesse\\OneDrive\\Desktop\\Research\\PD\\decline'\n",
    "folder = os.listdir(path)\n",
    "\n",
    "target_size = (96, 96, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"GET 100 OF CN AND PD IMAGES\"\n",
    "#\"MAKE 2 CLASSES\"\n",
    "\"USE BRAIN PATCHES\"\n",
    "#\"VALIDATION DATA BY SPLITTING TEST_TRAIN_SPLIT AGAIN + STRATIFY\"\n",
    "\n",
    "#First look at the learning curve to see if it does actually fit something.\n",
    "\n",
    "#Secondly, you use 0.2 of the dataset for a dataset of 20 images of 5 classes. \n",
    "#If all of your last images are of the same label. You will only test on that label. \n",
    "#So that might be one issue here unless the images aren't sorted.\n",
    "\n",
    "#Thirdly, for the few data it looks like you might have a lot of dense parameters. \n",
    "#Usually start small and increase the number of parameters. \n",
    "#You can see some hint about that by watching the learning curve.\n",
    "\n",
    "#Lastly, sadly machine learning is not magical and you can't expect good results with that few data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating x - converting images to array\n",
    "def read_image(path, folder):\n",
    "    mri = []\n",
    "    for i in range(len(folder)):\n",
    "        files = os.listdir(path + '\\\\' + folder[i])\n",
    "        for j in range(len(files)):\n",
    "            image = np.array(nib.load(path + '\\\\' + folder[i] + '\\\\' + files[j]).get_fdata())\n",
    "            image = np.resize(image, target_size)\n",
    "            image = np.expand_dims(image, axis=3)\n",
    "            image /= 255.\n",
    "            mri.append(image)\n",
    "    return mri\n",
    "\n",
    "# creating y - one hot encoder\n",
    "def create_y():\n",
    "    excel_file = r'C:\\Users\\jesse\\OneDrive\\Desktop\\Research\\PD\\decline_label.xlsx'\n",
    "    excel_read = pd.read_excel(excel_file)\n",
    "    excel_array = np.array(excel_read['Label'])\n",
    "    label = LabelEncoder().fit_transform(excel_array)\n",
    "    label = label.reshape(len(label), 1)\n",
    "    onehot = OneHotEncoder(sparse=False).fit_transform(label)\n",
    "    return onehot\n",
    "\n",
    "# Splitting image train/test\n",
    "x = np.asarray(read_image(path, folder))\n",
    "y = np.asarray(create_y())\n",
    "x_split, x_test, y_split, y_test = train_test_split(x, y, test_size=.2, stratify=y)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_split, y_split, test_size=.25, stratify=y_split)\n",
    "print(x_train.shape, x_val.shape, x_test.shape, y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = len(folder)\n",
    "\n",
    "inputs = Input((96, 96, 96, 1))\n",
    "conv1 = Conv3D(32, [3, 3, 3], padding='same', activation='relu')(inputs)\n",
    "conv1 = Conv3D(32, [3, 3, 3], padding='same', activation='relu')(conv1)\n",
    "pool1 = MaxPooling3D(pool_size=(2, 2, 2), padding='same')(conv1)\n",
    "#drop1 = Dropout(0.5)(pool1)\n",
    "\n",
    "conv2 = Conv3D(64, [3, 3, 3], padding='same', activation='relu')(pool1)\n",
    "conv2 = Conv3D(64, [3, 3, 3], padding='same', activation='relu')(conv2)\n",
    "pool2 = MaxPooling3D(pool_size=(2, 2, 2), padding='same')(conv2)\n",
    "#drop2 = Dropout(0.5)(pool2)\n",
    "\n",
    "conv3 = Conv3D(128, [3, 3, 3], padding='same', activation='relu')(pool2)\n",
    "conv3 = Conv3D(128, [3, 3, 3], padding='same', activation='relu')(conv3)\n",
    "pool3 = MaxPooling3D(pool_size=(2, 2, 2), padding='same')(conv3)\n",
    "#drop3 = Dropout(0.5)(pool3)\n",
    "\n",
    "conv4 = Conv3D(128, [3, 3, 3], padding='same', activation='relu')(pool3)\n",
    "conv4 = Conv3D(128, [3, 3, 3], padding='same', activation='relu')(conv4)\n",
    "pool4 = MaxPooling3D(pool_size=(2, 2, 2), padding='same')(conv4)\n",
    "#drop3 = Dropout(0.5)(pool3)\n",
    "\n",
    "flat1 = Flatten()(pool4)\n",
    "dense1 = Dense(128, activation='relu')(flat1)\n",
    "dense2 = Dense(32, activation='relu')(dense1)\n",
    "drop5 = Dropout(0.5)(dense2)\n",
    "dense3 = Dense(num_classes, activation='sigmoid')(drop5)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[dense3])\n",
    "\n",
    "opt = optimizers.Adam(lr=1e-4)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_datagen = customImageDataGenerator(horizontal_flip=True)\n",
    "\n",
    "val_datagen = customImageDataGenerator()\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "validation_set = val_datagen.flow(x_val, y_val, batch_size=batch_size)\n",
    "\n",
    "\n",
    "callbacks = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model.fit_generator(training_set,\n",
    "                    steps_per_epoch = 10,\n",
    "                    epochs = 50,\n",
    "                    validation_steps = 5,\n",
    "                    #callbacks = [callbacks],\n",
    "                    validation_data = validation_set)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=batch_size)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "map = sns.heatmap(confusion, annot=True)\n",
    "print(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(acc)\n",
    "plt.plot(val_acc)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.title('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
